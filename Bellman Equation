import numpy as np

num_states = 5 
num_actions = 3  
gamma = 0.9 
theta = 0.01  


V = np.zeros(num_states)


rewards = np.array([
    [1, -1, 0], 
    [0, 1, -1], 
    [1, 0, -1], 
    [-1, 1, 0],
    [0, -1, 1]   
])


transitions = np.array([
    [0, 1, 2],  # From state 0 to states 0, 1, 2
    [1, 2, 3],  # From state 1 to states 1, 2, 3
    [2, 3, 4],  # From state 2 to states 2, 3, 4
    [3, 4, 0],  # From state 3 to states 3, 4, 0
    [4, 0, 1]   # From state 4 to states 4, 0, 1
])


while True:
    delta = 0
    for s in range(num_states):
        v = V[s]
        # Calculate the expected value for all actions
        action_values = []
        for a in range(num_actions):
            action_value = rewards[s, a] + gamma * V[transitions[s, a]]
            action_values.append(action_value)
        V[s] = max(action_values)
        delta = max(delta, abs(v - V[s]))

    if delta < theta:
        break


print("Final State Values:")
print(V)


state = 0  
best_action = np.argmax([
    rewards[state, a] + gamma * V[transitions[state, a]]
    for a in range(num_actions)
])
print(f"Best Action for State {state}: {best_action}")
